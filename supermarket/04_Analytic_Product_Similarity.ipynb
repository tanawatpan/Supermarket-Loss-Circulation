{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Recommendation Based on Product Similarity\n",
    "\n",
    "We can help business understand more about their products by calculating product similarity (cosine similarity) based on user purchase data for particular item, then we can use product similarity to recommend similar product for users.\n",
    "\n",
    "Data\n",
    "* Customers X Products Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from itertools import chain\n",
    "\n",
    "ToArray = udf(lambda row: row.toArray().tolist() ,  ArrayType( DoubleType() , containsNull=False ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"SuperMarket Analytic\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Customer x Product data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM default.pivot_cust_prod_sum\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore = ['CUST_CODE']\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in df.columns if x not in ignore],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "# assemble feature into one column.\n",
    "data = assembler.transform(df).select('CUST_CODE', 'features').cache()\n",
    "\n",
    "# Normalize Feature\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=2.0)\n",
    "normalize_data = normalizer.transform(data).select(\"CUST_CODE\", \"normFeatures\")\n",
    "normalize_data = normalize_data.withColumn(\"normFeatures\", ToArray( normalize_data.normFeatures ) ).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|     CUST_CODE|        normFeatures|\n",
      "+--------------+--------------------+\n",
      "|CUST0000336458|[0.0, 0.0, 0.0, 0...|\n",
      "|CUST0000063499|[0.0, 0.0, 0.0, 0...|\n",
      "|CUST0000032037|[0.0, 0.0, 0.0, 0...|\n",
      "|CUST0000895912|[0.0, 0.0, 0.0, 0...|\n",
      "|CUST0000344205|[0.0, 0.0, 0.0, 0...|\n",
      "+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normalize_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cosine similarities between columns (Product) of this matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowmatrix = RowMatrix( normalize_data.select('normFeatures').rdd.map(list) )\n",
    "entries = rowmatrix.columnSimilarities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MatrixEntry to Dataframe\n",
    "result_rdd = entries.entries.map(lambda entry:  (entry.i, entry.j, entry.value) )\n",
    "result_df = spark.createDataFrame(result_rdd, ['I', 'J', 'Value']).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------------------+\n",
      "|   I|   J|               Value|\n",
      "+----+----+--------------------+\n",
      "| 372|2024|2.134202616595427...|\n",
      "| 918|1768|4.496209982804470...|\n",
      "|1221|4838|0.001437065595868...|\n",
      "|4762|4841|0.006215728928976862|\n",
      "|2286|3181|7.723494644030827E-4|\n",
      "| 765|3556| 0.01909570156185527|\n",
      "| 973|2118|5.565617584170745E-4|\n",
      "|3243|4070|7.693018437447457E-4|\n",
      "| 320| 788|0.002723796615577...|\n",
      "| 999|4472|3.395537150058167...|\n",
      "+----+----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [x for x in df.columns if x not in ignore]\n",
    "dict_columns = { i:columns[i] for i in range( len(columns) )}\n",
    "\n",
    "MapIndexWithName = create_map([lit(x) for x in chain(*dict_columns.items() ) ])\n",
    "\n",
    "# Map Column Index with Column Name\n",
    "Product_Similarity = result_df.withColumn(\"Product1\", MapIndexWithName.getItem( col(\"I\") ) ) \\\n",
    "                              .withColumn(\"Product2\", MapIndexWithName.getItem( col(\"J\") ) ) \\\n",
    "                              .select(\"Product1\", \"Product2\", \"Value\") \\\n",
    "                              .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 15 Product that are simililar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------+\n",
      "|  Product1|  Product2|             Value|\n",
      "+----------+----------+------------------+\n",
      "|PRD0900831|PRD0902580| 0.983213862373871|\n",
      "|PRD0901814|PRD0903406|0.9773287691165502|\n",
      "|PRD0902237|PRD0903643|0.9484370041654808|\n",
      "|PRD0900585|PRD0901208|0.9462486961404734|\n",
      "|PRD0901517|PRD0904708|0.9455417399357372|\n",
      "|PRD0900918|PRD0904576|0.9398997657902919|\n",
      "|PRD0903843|PRD0904187|0.9356220652495086|\n",
      "|PRD0901061|PRD0901960|0.9218063568108338|\n",
      "|PRD0904677|PRD0904778|0.9216428116462088|\n",
      "|PRD0900350|PRD0904765| 0.914648521705544|\n",
      "|PRD0900216|PRD0901177|0.9144144765220199|\n",
      "|PRD0900861|PRD0904694|0.9124786224521093|\n",
      "|PRD0901593|PRD0903305|0.9104669706293007|\n",
      "|PRD0901005|PRD0903807|0.9035063940391864|\n",
      "|PRD0903946|PRD0904718|0.9034095106847959|\n",
      "+----------+----------+------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Product_Similarity.orderBy( col(\"Value\").desc() ).show(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}